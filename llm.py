#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BGE-m3 RAG ê²€ìƒ‰ + LLM ë‹µë³€ ì‹œìŠ¤í…œ (UI/ìµœì í™” ì—…ê·¸ë ˆì´ë“œ)
- ìµœì‹  LLM ì„ íƒ ì§€ì›(ë“œë¡­ë‹¤ìš´ + ì»¤ìŠ¤í…€ ëª¨ë¸)
- ì°¸ì¡° ë¬¸ì„œ ìˆ˜ AUTO ìµœì í™”(ê¸°ë³¸)
- ì°¸ì¡° ë¬¸ì„œ ìƒì„¸: ëª¨ë‘ í¼ì¹¨ + ê°€ë…ì„± ê°•í™” + í‘œ ì „ì²´ í‘œì‹œ
- ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ ì—„ê²©í™”(ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê°•í™”)
"""

import os
os.environ['HF_HUB_DISABLE_SYMLINKS'] = '1'

import json
import time
from pathlib import Path
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

import numpy as np
import pandas as pd
import streamlit as st

try:
    import scipy.sparse as sp
    SCIPY_AVAILABLE = True
except:
    SCIPY_AVAILABLE = False

try:
    from FlagEmbedding import BGEM3FlagModel
    BGE_AVAILABLE = True
except:
    BGE_AVAILABLE = False

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except:
    OPENAI_AVAILABLE = False

load_dotenv()

st.set_page_config(
    page_title="ì„¤ê³„ì‹¤ë¬´ì§€ì¹¨ AI ê²€ìƒ‰",
    page_icon="ğŸ—ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ------------------------------
# ìŠ¤íƒ€ì¼ (ê°€ë…ì„± ì—…ê·¸ë ˆì´ë“œ)
# ------------------------------
st.markdown("""
<style>
  .main { background-color: #f8f9fa; }
  .stButton > button { width: 100%; }
  .answer-box {
    background: #ffffff;
    padding: 1.25rem 1.5rem;
    border-radius: 12px;
    border: 1px solid #e9ecef;
    box-shadow: 0 1px 2px rgba(0,0,0,0.03);
  }
  .result-card {
    background: #ffffff;
    padding: 1rem 1.1rem;
    border-radius: 12px;
    border: 1px solid #e9ecef;
    margin: 0.6rem 0;
  }
  .result-header {
    display: flex; justify-content: space-between; align-items: baseline;
    margin-bottom: 0.4rem;
  }
  .pill {
    display:inline-block; padding: 0.12rem 0.5rem; border-radius: 999px;
    border:1px solid #dee2e6; background:#f8f9fa; font-size: 0.8rem; color:#495057;
    margin-right: 0.4rem;
  }
  .meta {
    font-size: 0.85rem; color: #6c757d; margin-top: 0.15rem;
  }
  .codebox {
    background:#fcfcfd; border:1px solid #eee; border-radius:10px; padding:0.75rem;
    font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", monospace;
    font-size: 0.9rem; white-space: pre-wrap;
  }
  /* ë°ì´í„°í”„ë ˆì„ ë„ˆë¹„ í™•ë³´ */
  .block-container { padding-top: 1.2rem; padding-bottom: 3rem; }
</style>
""", unsafe_allow_html=True)

# =========================
# ì„ë² ë”© ë§¤ë‹ˆì € (ê°„ì†Œí™”)
# =========================
class EmbeddingManager:
    def __init__(self):
        self.model: Optional[BGEM3FlagModel] = None
        self.client: Optional[OpenAI] = None
        self.embeddings_dense: Optional[np.ndarray] = None
        self.embeddings_dense_norm: Optional[np.ndarray] = None
        self.embeddings_sparse: Optional[Any] = None
        self.chunks: Optional[List[Dict[str, Any]]] = None
        self.embedding_dim: Optional[int] = None
        self.device = "cpu"
        self._qcache: Dict[str, Any] = {}

    @st.cache_resource(show_spinner=False)
    def load_bge_model(_self):
        if not BGE_AVAILABLE:
            return None
        device = "cpu"
        try:
            import torch
            if torch.cuda.is_available():
                device = "cuda"
        except:
            pass
        with st.spinner(f"ğŸ”„ BGE-m3 ëª¨ë¸ ë¡œë”©... (device={device})"):
            model = BGEM3FlagModel("BAAI/bge-m3", use_fp16=(device=="cuda"))
        _self.device = device
        return model

    def init_openai(self):
        if not OPENAI_AVAILABLE:
            st.error("âŒ openai íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            st.error("âŒ .env íŒŒì¼ì— OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤!")
            return None
        try:
            self.client = OpenAI(api_key=api_key)
            return self.client
        except Exception as e:
            st.error(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            return None

    def load_manifest(self, base_dir: Path) -> Dict[str, Any]:
        mp = base_dir / "manifest.json"
        if mp.exists():
            with open(mp, "r", encoding="utf-8") as f:
                return json.load(f)
        return {"shards": {}}

    def load_shard_npz(self, base_dir: Path, shard_name: str):
        try:
            mani = self.load_manifest(base_dir)
            if shard_name not in mani.get("shards", {}):
                raise RuntimeError(f"ìƒ¤ë“œ '{shard_name}' ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            meta = mani["shards"][shard_name]

            # Dense
            npz_file = base_dir / meta["dense"]["file"]
            with np.load(npz_file, allow_pickle=True) as npz:
                self.embeddings_dense = np.array(npz["dense"])
                self.embeddings_dense_norm = np.array(npz["dense_norm"])
                self.embedding_dim = int(npz["dim"][0])
                self.device = str(npz["device"][0])

            # Sparse
            sparse_meta = meta["sparse"]
            if sparse_meta["type"] == "csr" and SCIPY_AVAILABLE:
                csr_file = base_dir / sparse_meta["file"]
                self.embeddings_sparse = sp.load_npz(csr_file).tocsr().astype(np.float32)
            else:
                with open(base_dir / sparse_meta["file"], "r", encoding="utf-8") as f:
                    js = json.load(f)
                self.embeddings_sparse = {int(k): {int(kk): float(vv) for kk, vv in d.items()}
                                          for k, d in js.items()}

            # Chunks
            chunks_file = base_dir / meta["chunks"]
            self.chunks = []
            with open(chunks_file, "r", encoding="utf-8") as f:
                for line in f:
                    if line.strip():
                        self.chunks.append(json.loads(line))

            self._qcache.clear()

        except Exception as e:
            st.error(f"âŒ ìƒ¤ë“œ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            raise

    def encode_query(self, query: str, max_len: int = 512):
        key = f"{query}|{max_len}"
        if key in self._qcache:
            return self._qcache[key]["dense"], self._qcache[key]["sparse"]

        if not self.model:
            self.model = self.load_bge_model()
        if not self.model:
            st.error("âŒ BGE-m3 ëª¨ë¸ì„ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            raise RuntimeError("BGE model unavailable")

        out = self.model.encode(
            [query], batch_size=1, max_length=max_len,
            return_dense=True, return_sparse=True, return_colbert_vecs=False
        )

        q_dense = out["dense_vecs"][0].astype(np.float32)
        q_dense /= (np.linalg.norm(q_dense) + 1e-12)

        q_sparse = out.get("lexical_weights", [{}])[0]
        q_sparse = {int(k): float(v) for k, v in q_sparse.items()}

        self._qcache[key] = {"dense": q_dense, "sparse": q_sparse}
        return q_dense, q_sparse

    def _dense_scores(self, q_dense: np.ndarray) -> np.ndarray:
        return self.embeddings_dense_norm @ q_dense

    def _sparse_scores(self, q_sparse: Dict[int, float]) -> np.ndarray:
        n = len(self.chunks)
        if not q_sparse:
            return np.zeros(n, dtype=np.float32)

        if isinstance(self.embeddings_sparse, sp.csr_matrix):
            q_vec = np.zeros(self.embeddings_sparse.shape[1], dtype=np.float32)
            for tid, weight in q_sparse.items():
                if tid < len(q_vec):
                    q_vec[tid] = weight
            return np.array(self.embeddings_sparse @ q_vec).flatten()

        scores = np.zeros(n, dtype=np.float32)
        for idx, doc_sparse in self.embeddings_sparse.items():
            s = 0.0
            for tid, qw in q_sparse.items():
                dv = doc_sparse.get(tid)
                if dv is not None:
                    s += qw * dv
            scores[idx] = s
        return scores

    def search(self, query: str, top_k: int = 10, mode: str = "hybrid",
               dense_weight: float = 0.6) -> List[Dict[str, Any]]:
        if self.embeddings_dense_norm is None or self.chunks is None:
            return []

        q_dense, q_sparse = self.encode_query(query)

        if mode == "dense":
            scores = self._dense_scores(q_dense)
        elif mode == "sparse":
            scores = self._sparse_scores(q_sparse)
        else:  # hybrid
            dscore = self._dense_scores(q_dense)
            sscore = self._sparse_scores(q_sparse)
            # ê°„ë‹¨ í‘œì¤€í™” í›„ ê°€ì¤‘í•©
            dscore = (dscore - dscore.mean()) / (dscore.std() + 1e-12)
            sscore = (sscore - sscore.mean()) / (sscore.std() + 1e-12)
            scores = dense_weight * dscore + (1 - dense_weight) * sscore

        # í‘œ ë¶€ìŠ¤íŒ…(ì§ˆë¬¸ì— í‘œ/ìˆ˜ì¹˜ ê´€ë ¨ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ í‘œ í¬í•¨ ì²­í¬ ê°€ì¤‘)
        table_terms = ["í‘œ", "êµ¬ë¶„", "ì—°ë½ì²˜", "ì „í™”", "ë¶€ì„œ", "ë…¸ì„ ", "kN", "MPa", "mm", "ê¸°ì¤€", "ìˆ˜ì¹˜"]
        if any(t in query for t in table_terms):
            boost = np.array([1.05 if c.get("metadata", {}).get("has_table") else 1.0
                              for c in self.chunks], dtype=np.float32)
            scores = scores * boost

        top_idx = np.argsort(scores)[::-1][:top_k]
        results = []
        for rank, idx in enumerate(top_idx, 1):
            results.append({
                "rank": rank,
                "index": int(idx),
                "score": float(scores[idx]),
                "chunk": self.chunks[idx]
            })
        return results

# =========================
# ìœ í‹¸: ì»¨í…ìŠ¤íŠ¸/í† í°/ì˜¤í† -K
# =========================
def approx_tokens(text: str) -> int:
    # ëŒ€ëµ 1í† í° â‰ˆ 4ì(í•œê¸€/ì˜ë¬¸ í˜¼ì¬ ê¸°ì¤€ ëŒ€ì¶©ì¹˜)
    return max(1, len(text) // 4)

def build_context_from_results(results: List[Dict[str, Any]], max_chunks: int = 5) -> str:
    context_parts = []
    for i, result in enumerate(results[:max_chunks], 1):
        chunk = result["chunk"]

        heading_parts = []
        if chunk.get("big_heading"):
            heading_parts.append(str(chunk["big_heading"]))
        if chunk.get("mid_heading") and chunk["mid_heading"] != chunk.get("big_heading"):
            heading_parts.append(str(chunk["mid_heading"]))
        heading = " > ".join(heading_parts) if heading_parts else "ì œëª© ì—†ìŒ"

        items = chunk.get("items", []) or []
        text_parts = []

        for item in items:
            if not isinstance(item, dict):
                continue
            if item.get("type") == "text":
                txt = (item.get("content") or "").strip()
                if txt:
                    text_parts.append(txt)
            elif item.get("type") == "table":
                title = item.get("title", "í‘œ")
                tbl = item.get("table_data", {})
                cols = [str(c) for c in tbl.get("columns", [])]
                rows = tbl.get("data", [])
                table_str = f"\n[{title}]\n"
                if cols:
                    table_str += " | ".join(cols) + "\n"
                for row in rows:
                    table_str += " | ".join([str(x) if x is not None else "" for x in row]) + "\n"
                text_parts.append(table_str)

        content = "\n".join(text_parts) if text_parts else (chunk.get("content", "")[:2000])

        meta = chunk.get("metadata", {})
        source = meta.get("source_file", meta.get("file_name", "unknown"))

        context_parts.append(
            f"[ë¬¸ì„œ {i}]\n"
            f"ì¶œì²˜: {source}\n"
            f"ì œëª©: {heading}\n"
            f"ë‚´ìš©:\n{content}\n"
            f"{'='*60}"
        )
    return "\n\n".join(context_parts)

def choose_auto_context_k(query: str, results: List[Dict[str, Any]], max_cap: int = 10) -> int:
    """
    ê°„ë‹¨ íœ´ë¦¬ìŠ¤í‹±:
    - ê¸°ë³¸ 6ê°œ
    - ì§ˆë¬¸ì´ ê¸¸ìˆ˜ë¡ +1 (>=80ì)
    - í‘œ/ìˆ˜ì¹˜ í‚¤ì›Œë“œ í¬í•¨ ì‹œ +1
    - ê²°ê³¼ê°€ ë§¤ìš° ì ìœ¼ë©´ len(results)ë¡œ ì œí•œ
    - ìƒí•œ 10
    """
    k = 6
    if len(query) >= 80:
        k += 1
    if any(t in query for t in ["í‘œ", "ê¸°ì¤€", "ìˆ˜ì¹˜", "ì ˆì°¨", "ë‹¨ê³„", "MPa", "kN", "mm"]):
        k += 1
    k = min(k, len(results), max_cap)
    return max(1, k)

# =========================
# LLM í•¨ìˆ˜ë“¤
# =========================
STRICT_SYSTEM_PROMPT = """
ë‹¹ì‹ ì€ í•œêµ­ë„ë¡œê³µì‚¬ 'ì„¤ê³„ì‹¤ë¬´ì§€ì¹¨' ì „ë¬¸ê°€ë‹¤. ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µí•˜ë¼.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì •/ë°œëª…/ì¼ë°˜ìƒì‹ ë³´ì¶©ì„ ê¸ˆì§€í•œë‹¤.
ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ì •í™•íˆ ë‹¤ìŒ ë¬¸êµ¬ë¡œ ë‹µí•œë‹¤: "ì œê³µëœ ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

[ì¶œë ¥ í˜•ì‹(ì›¹ ê°€ë…ì„± ìµœì í™”)]
- Markdownì„ ì‚¬ìš©í•œë‹¤.
- ìƒë‹¨ì— ìš”ì•½(3~5ì¤„) â†’ ê·¸ ì•„ë˜ì— ì„¸ë¶€ë‚´ìš©ì„ ë²ˆí˜¸ ëª©ë¡ìœ¼ë¡œ ë‹¨ê³„/ì ˆì°¨/ì¡°ê±´ì„ ì •ë¦¬í•œë‹¤.
- ìˆ˜ì¹˜Â·ê¸°ì¤€ì€ ë‹¨ìœ„ì™€ ë²”ìœ„ë¥¼ ë°˜ë“œì‹œ í•¨ê»˜ ì ëŠ”ë‹¤(ì˜ˆ: 1.5 m, 30 MPa, Â±5%).
- í‘œê°€ í•„ìš”í•˜ë©´:
  1) ë¬¸ì„œ í‘œì˜ í•µì‹¬ í–‰/ì—´ë§Œ ë½‘ì•„ **Markdown í‘œ**ë¡œ ì¬êµ¬ì„±í•˜ê±°ë‚˜,
  2) ì›ë³¸ í‘œê°€ ê·¸ëŒ€ë¡œ ì „ë‹¬ë˜ì–´ì•¼ ì˜ë¯¸ê°€ ë³´ì¡´ë  ë•ŒëŠ” **"ì›ë³¸ í‘œ ê·¸ëŒ€ë¡œ"**ë¥¼ í‘œì‹œí•˜ê³  í‘œ ë‚´ìš©ì„ ê°€ëŠ¥í•œ í•œ ê·¸ëŒ€ë¡œ ì œì‹œí•œë‹¤.
- ê¸´ í•­ëª©ì€ í•˜ìœ„ ë¶ˆë¦¿(-)ì„ í™œìš©í•´ ì¤„ë°”ê¿ˆê³¼ ë“¤ì—¬ì“°ê¸°ë¡œ ê°€ë…ì„±ì„ ë†’ì¸ë‹¤.
- ë§ˆì§€ë§‰ì—ëŠ” **ì°¸ì¡°** ì„¹ì…˜ì„ ë„£ë˜, [ë¬¸ì„œ 1] ê°™ì€ ë²ˆí˜¸ í‘œê¸°ëŠ” ê¸ˆì§€í•˜ê³ ,
  ê° ê·¼ê±°ë³„ë¡œ **ì¶œì²˜ íŒŒì¼ëª… Â· í°ì œëª© Â· ì¤‘ê°„ì œëª©**ì„ â€˜Â·â€™ë¡œ êµ¬ë¶„í•´ ë‚˜ì—´í•œë‹¤.
  (ì˜ˆì‹œ: ì„¤ê³„ì‹¤ë¬´ì§€ì¹¨_2019.pdf Â· 3. ë„ë¡œêµ¬ì¡°ë¬¼ ì„¤ê³„ê¸°ì¤€ Â· 3.2 í•˜ì¤‘ì¡°í•©)

[ë‚´ìš© ì‘ì„± ê·œì¹™]
1) ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ì¡°í•­, ì ˆì°¨, ì±…ì„ì£¼ì²´, ì‹œê¸°, ì˜ˆì™¸ì¡°ê±´ì„ **ìš°ì„ ìˆœìœ„**ë¡œ ì •ë¦¬í•œë‹¤.
2) ì„œë¡œ ìƒì¶©í•˜ëŠ” ê¸°ìˆ  ê¸°ì¤€ì´ ìˆìœ¼ë©´ **ê·¼ê±°ë³„ë¡œ ë¶„ë¦¬**í•˜ì—¬ ë³‘ê¸°í•˜ê³ , ì ìš© ì¡°ê±´ì„ ëª…ì‹œí•œë‹¤.
3) í‘œ/ëª©ë¡ì— í¬í•¨ë˜ëŠ” ìˆ˜ì¹˜ëŠ” **ì›ë¬¸ ìˆ˜ì¹˜ ê·¸ëŒ€ë¡œ** ì‚¬ìš©í•˜ë©°, ë‹¨ìœ„ë¥¼ ëˆ„ë½í•˜ì§€ ì•ŠëŠ”ë‹¤.
4) ìˆ˜ì‹/ê¸°í˜¸ê°€ ë“±ì¥í•˜ë©´ ê°„ë‹¨íˆ í’€ì–´ì“´ ì„¤ëª…ì„ í•¨ê»˜ ë¶™ì¸ë‹¤.
5) ë‹µë³€ ë³¸ë¬¸ ì¤‘ ë¶ˆê°€í”¼í•œ ì¸ìš©ì€ ì§§ê²Œ ìš”ì•½ ì¸ìš©ë§Œ í•˜ê³ (ë‘ì„¸ ì¤„ ì´ë‚´), ì›ë¬¸ ì „ì²´ ì œì‹œëŠ” í”¼í•œë‹¤.
6) ì œê³µ ì»¨í…ìŠ¤íŠ¸ ì™¸ ë°°ê²½ì§€ì‹, íƒ€ ê·œì •/ì½”ë“œ, ì¼ë°˜ ìƒì‹, ê°œì¸ì  íŒë‹¨ì€ ì‚¬ìš© ê¸ˆì§€.

[ì°¸ì¡° í‘œê¸° ë°©ë²•(í•„ìˆ˜)]
- ë³¸ë¬¸ ë§ˆì§€ë§‰ì— ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‚˜ì—´:
  - ì¶œì²˜ íŒŒì¼ëª… Â· í°ì œëª© Â· ì¤‘ê°„ì œëª©
  - (ì—¬ëŸ¬ ê°œë©´ ì¤„ë°”ê¿ˆìœ¼ë¡œ ê°ê° ë³„ë„ í‘œê¸°)
"""


def generate_llm_response_streaming(client: OpenAI, query: str, context: str,
                                    model: str, placeholder) -> str:
    user_prompt = f"""ì§ˆë¬¸: {query}

ì°¸ì¡° ë¬¸ì„œ:
{context}

ìœ„ ë¬¸ì„œì—ì„œ ê·¼ê±°ë¥¼ ì°¾ì•„ ì§ˆë¬¸ì—ë§Œ ë‹µí•´ë¼. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ë‹µí•˜ì§€ ë§ë¼.
"""

    try:
        stream = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": STRICT_SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.2,
            max_completion_tokens=3000,
            stream=True
        )
        response_text = ""
        for chunk in stream:
            if chunk.choices and chunk.choices[0].delta and chunk.choices[0].delta.content:
                response_text += chunk.choices[0].delta.content
                placeholder.markdown(response_text + "â–Œ")
        placeholder.markdown(response_text)
        return response_text
    except Exception as e:
        placeholder.error(f"âŒ LLM í˜¸ì¶œ ì˜¤ë¥˜: {str(e)}")
        return ""

# =========================
# UI: ì°¸ì¡° ë¬¸ì„œ ìƒì„¸(ëª¨ë‘ í¼ì¹¨)
# =========================
def render_chunk_full(result: Dict[str, Any]):
    idx = result["index"]
    score = result["score"]
    chunk = result["chunk"]

    heading_parts = []
    if chunk.get("big_heading"):
        heading_parts.append(str(chunk["big_heading"]))
    if chunk.get("mid_heading") and chunk["mid_heading"] != chunk.get("big_heading"):
        heading_parts.append(str(chunk["mid_heading"]))
    heading = " > ".join(heading_parts) if heading_parts else "ì œëª© ì—†ìŒ"

    meta = chunk.get("metadata", {})
    source_folder = meta.get("source_folder", "N/A")
    source_file = meta.get("source_file", meta.get("file_name", "N/A"))
    page = meta.get("page", None)

    st.markdown('<div class="result-card">', unsafe_allow_html=True)
    st.markdown(
        f'<div class="result-header"><div>'
        f'<span class="pill">ì²­í¬ #{idx}</span>'
        f'<span class="pill">ì ìˆ˜ {score:.4f}</span>'
        f'</div><div style="font-weight:600;">ğŸ“Œ {heading}</div></div>',
        unsafe_allow_html=True
    )
    st.markdown(
        f'<div class="meta">ğŸ“ {source_folder} &nbsp;&middot;&nbsp; '
        f'ğŸ“„ {source_file}'
        + (f' &nbsp;&middot;&nbsp; ğŸ“ƒ p.{page}' if page is not None else '')
        + '</div>',
        unsafe_allow_html=True
    )

    st.markdown("---")

    items = chunk.get("items", []) or []
    # ëª¨ë“  ë‚´ìš© í‘œì‹œ(í‘œëŠ” ì „ì²´, í…ìŠ¤íŠ¸ëŠ” ëª¨ë‘)
    for j, item in enumerate(items, 1):
        if not isinstance(item, dict):
            continue
        if item.get("type") == "text":
            txt = (item.get("content") or "").strip()
            if txt:
                st.markdown(txt)
        elif item.get("type") == "table":
            title = item.get("title", "í‘œ")
            tbl = item.get("table_data", {})
            df = pd.DataFrame(tbl.get("data", []))
            cols = [str(c) for c in tbl.get("columns", [])]
            if cols and len(cols) == df.shape[1]:
                df.columns = cols
            st.markdown(f"**ğŸ“Š {title}**")
            st.dataframe(df, use_container_width=True)
        elif item.get("type") == "code":
            code_text = item.get("content", "")
            st.markdown(f"**ğŸ”§ ì½”ë“œ**")
            st.markdown(f"<div class='codebox'>{code_text}</div>", unsafe_allow_html=True)
        else:
            # ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì…ì€ ë¬¸ìì—´í™”
            st.json(item)
    st.markdown('</div>', unsafe_allow_html=True)

# =========================
# ë©”ì¸
# =========================
def main():
    st.title("ğŸ—ï¸ ì„¤ê³„ì‹¤ë¬´ì§€ì¹¨ AI ê²€ìƒ‰")
    st.caption("BGE-m3 ì„ë² ë”© + GPT ë‹µë³€ ìƒì„±")

    # ì´ˆê¸°í™”
    if "manager" not in st.session_state:
        st.session_state.manager = EmbeddingManager()
    manager: EmbeddingManager = st.session_state.manager

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")

        # OpenAI
        if st.button("ğŸ”‘ OpenAI ì—°ê²° í™•ì¸"):
            if manager.init_openai():
                st.success("âœ… ì—°ê²° ì„±ê³µ!")

        st.divider()

        # ìƒ¤ë“œ ë¡œë“œ
        st.subheader("ğŸ“‚ ì„ë² ë”© ë¡œë“œ")
        base_dir = Path(st.text_input("ìƒ¤ë“œ í´ë”", value=".embeddings_shards"))
        if not base_dir.exists():
            st.error("âŒ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
            st.stop()

        mani = manager.load_manifest(base_dir)
        shard_names = list(mani.get("shards", {}).keys())
        if not shard_names:
            st.warning("âš ï¸ ìƒ¤ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

        selected_shard = st.selectbox("ìƒ¤ë“œ ì„ íƒ", shard_names)
        if st.button("ğŸ”„ ìƒ¤ë“œ ë¡œë“œ", type="primary"):
            with st.spinner("ë¡œë”© ì¤‘..."):
                manager.load_shard_npz(base_dir, selected_shard)
            st.success(f"âœ… {len(manager.chunks):,}ê°œ ì²­í¬ ë¡œë“œ!")

        st.divider()

        # LLM ì„¤ì • (ìµœì‹  ì„ íƒ + ì»¤ìŠ¤í…€)
        st.subheader("ğŸ¤– LLM ì„¤ì •")
        # ë“œë¡­ë‹¤ìš´ í›„ë³´ëŠ” ì˜ˆì‹œì´ë©°, ê³„ì •ì—ì„œ ì§€ì›ë˜ëŠ” ìµœì‹  ëª¨ë¸ëª…ì„ ì»¤ìŠ¤í…€ ì…ë ¥ìœ¼ë¡œ ì¶”ê°€ ì‚¬ìš© ê°€ëŠ¥
        model_options = {
            "GPT-5 (2025ë…„ ìµœì‹ )": "gpt-5",
            "GPT-5-mini (ê²½ëŸ‰ GPT-5)": "gpt-5-mini",
            "GPT-5-nano (ì´ˆê²½ëŸ‰)": "gpt-5-nano",
            "o1 (ì¶”ë¡  ëª¨ë¸)": "o1",
            "o1-preview": "o1-preview",
            "o1-mini": "o1-mini",
            "GPT-4o (ì•ˆì •)": "gpt-4o",
            "GPT-4o-mini (ì¶”ì²œ/ì €ë¹„ìš©)": "gpt-4o-mini",  # â­ ê¸°ë³¸ê°’
            "GPT-4-turbo": "gpt-4-turbo"
        }
        model_display = st.selectbox("ëª¨ë¸", list(model_options.keys()), index=6)
        selected_model = model_options[model_display]
        custom_model = st.text_input("ì¶”ê°€/ì»¤ìŠ¤í…€ ëª¨ë¸ëª…(ì„ íƒ)", value="", placeholder="ì˜ˆ: gpt-5.1, gpt-5o-reasoning ë“±")
        if custom_model.strip():
            selected_model = custom_model.strip()

        st.divider()

        # ê²€ìƒ‰ ì„¤ì •
        st.subheader("ğŸ” ê²€ìƒ‰ ì„¤ì •")
        search_mode = st.radio("ëª¨ë“œ", ["Hybrid", "Dense", "Sparse"], index=0, horizontal=True)
        mode_map = {"Hybrid": "hybrid", "Dense": "dense", "Sparse": "sparse"}
        mode = mode_map[search_mode]
        dense_weight = st.slider("Dense ê°€ì¤‘ì¹˜", 0.0, 1.0, 0.6, 0.1) if mode == "hybrid" else 0.6
        top_k = st.slider("ê²€ìƒ‰ ê²°ê³¼ ìˆ˜(Top-K)", 3, 30, 12, 1)

        # ì°¸ì¡° ë¬¸ì„œ ìˆ˜: AUTO ê¸°ë³¸
        st.subheader("ğŸ§  ì°¸ì¡° ë¬¸ì„œ ìˆ˜")
        use_auto = st.checkbox("AUTO(ê¶Œì¥)", value=True,
                               help="ì§ˆë¬¸/í‚¤ì›Œë“œì— ë”°ë¼ 4~10ê°œ ì‚¬ì´ë¡œ ìë™ ì„ íƒ")
        context_chunks_manual = st.slider("ìˆ˜ë™ ì„¤ì •(ë¹„í™œì„± ì‹œ ë¬´ì‹œ)", 1, 15, 6, 1)

        st.divider()

        if manager.chunks:
            st.metric("ğŸ“Š ì²­í¬", f"{len(manager.chunks):,}")
            st.metric("ğŸ”¢ ì°¨ì›", manager.embedding_dim)

    # ë©”ì¸
    if manager.chunks is None:
        st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ìƒ¤ë“œë¥¼ ë¨¼ì € ë¡œë“œí•˜ì„¸ìš”!")
        st.stop()

    if manager.client is None:
        manager.init_openai()
        if manager.client is None:
            st.warning("âš ï¸ OpenAI API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”!")
            st.stop()

    # ì˜ˆì œ ì§ˆë¬¸
    st.subheader("ğŸ“ ì˜ˆì œ ì§ˆë¬¸")
    example_questions = [
        "ì„¤ê³„ ì•ˆì „ì„± ê²€í† ê°€ ë¬´ì—‡ì´ê³  ì–´ë–»ê²Œ ì ìš©í•˜ëŠ”ì§€ ì•Œë ¤ì¤˜",
        "ì„¤ê³„ë‹¨ê³„ë³„ ì£¼ë¯¼ì„¤ëª…íšŒë¥¼ ì–¸ì œ í•´ì•¼í•˜ëŠ”ì§€ ì•Œë ¤ì¤˜",
        "ì§€ì ì¤‘ì²©ë„ ì‘ì„± ì˜ë¢°ë¥¼ ì–¸ì œ í•´ì•¼í•˜ëŠ”ì§€ ì•Œë ¤ì¤˜",
        "í™•ì¥êµ¬ê°„ ë‚´ ì œí•œì†ë„ì™€ ìµœì†Œ ê³¡ì„ ë°˜ê²½ì„ ì•Œë ¤ì¤˜",
        "ì ê²€ ìŠ¹ê°•ì‹œì„¤ì´ ë¬´ì—‡ì´ê³  ì–´ë””ì— ì„¤ì¹˜í•´ì•¼ í•˜ëŠ”ì§€ ì•Œë ¤ì¤˜",
        "í˜„ê´‘ë°©ì§€ì‹œì„¤ ì„¤ì¹˜ë°©ë²•ì„ ì•Œë ¤ì¤˜",
        "í†µê³¼ë†’ì´ ì œí•œì‹œì„¤ ì„¤ì¹˜ê¸°ì¤€ ì•Œë ¤ì¤˜",
        "í’ˆì§ˆì‹œí—˜ì‚¬ ì¸ê±´ë¹„ ë°˜ì˜ ë°©ë²•ì„ ì•Œë ¤ì¤˜"
    ]
    cols = st.columns(4)
    for i, q in enumerate(example_questions):
        with cols[i % 4]:
            if st.button(f"Q{i+1}", key=f"ex_{i}", use_container_width=True, help=q):
                st.session_state.selected_query = q
                st.rerun()

    st.divider()

    # ê²€ìƒ‰ì°½
    default_query = st.session_state.get("selected_query", "")
    query = st.text_input("ğŸ” ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", value=default_query,
                          placeholder="ì˜ˆ: ì„¤ê³„ ì•ˆì „ì„± ê²€í† ë€?")
    if "selected_query" in st.session_state:
        del st.session_state.selected_query

    # ì‹¤í–‰
    if st.button("ğŸš€ ê²€ìƒ‰ & ë‹µë³€ ìƒì„±", type="primary", use_container_width=True) or default_query:
        if not query:
            st.warning("âš ï¸ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”!")
            st.stop()

        # 1) ê²€ìƒ‰
        with st.spinner("ğŸ” ë¬¸ì„œ ê²€ìƒ‰ ì¤‘..."):
            t0 = time.time()
            results = manager.search(query, top_k=top_k, mode=mode, dense_weight=dense_weight)
            search_time = time.time() - t0

        if not results:
            st.warning("âš ï¸ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()

        st.success(f"âœ… {len(results)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ ({search_time:.2f}ì´ˆ)")

        # 2) ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (AUTO ê¸°ë³¸)
        if use_auto:
            k_auto = choose_auto_context_k(query, results, max_cap=10)
            context = build_context_from_results(results, max_chunks=k_auto)
            chosen_k = k_auto
        else:
            context = build_context_from_results(results, max_chunks=context_chunks_manual)
            chosen_k = context_chunks_manual

        # 3) LLM ë‹µë³€
        st.markdown("### ğŸ¤– AI ë‹µë³€")
        answer_placeholder = st.empty()
        with st.spinner(f"ğŸ’­ {selected_model} ë‹µë³€ ìƒì„± ì¤‘..."):
            _ = generate_llm_response_streaming(
                manager.client, query, context, selected_model, answer_placeholder
            )

        # 4) ì°¸ì¡° ë¬¸ì„œ ìƒì„¸(ëª¨ë‘ í¼ì¹¨ + ê°€ë…ì„± ê°•í™”)
        st.markdown("---")
        st.markdown("### ğŸ“š ì°¸ì¡° ë¬¸ì„œ ìƒì„¸")
        for r in results:
            render_chunk_full(r)

        # í†µê³„
        st.markdown("---")
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("â±ï¸ ê²€ìƒ‰ ì‹œê°„", f"{search_time:.2f}ì´ˆ")
        with col2:
            st.metric("ğŸ“„ ê²€ìƒ‰ ê²°ê³¼", f"{len(results)}ê°œ")
        with col3:
            st.metric("ğŸ§  LLM ëª¨ë¸", selected_model)
        with col4:
            st.metric("ğŸ“š LLM ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ", f"{chosen_k}ê°œ")

if __name__ == "__main__":
    if not BGE_AVAILABLE:
        st.error("âŒ FlagEmbedding ë¯¸ì„¤ì¹˜")
        st.code("pip install FlagEmbedding")
        st.stop()
    if not OPENAI_AVAILABLE:
        st.error("âŒ openai íŒ¨í‚¤ì§€ ë¯¸ì„¤ì¹˜")
        st.code("pip install openai python-dotenv")
        st.stop()
    main()
